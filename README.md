# Perosnal-project:
# Research report analyze:

Developed an automated investment research pipeline leveraging LLMs and the Agently framework to generate comprehensive investment reports based on real-time financial news data. The project encompasses data collection, processing, and report generation through an AI-driven workflow. Key components include
*Technologies: Python, Agently, OpenAI API, MySQL, Web Scraping, Chain of Thought (CoT) Prompting*  

🚀 **Project Highlights**  

- 📰 **Automated news scraping** – Collected investment-related news articles from various financial sources using web scraping techniques.  
- 📊 **Detailed investment reports** – Generated comprehensive research reports with structured analysis of financial performance, risks, and growth opportunities.  
- 🧠 **LLM-powered analysis** – Leveraged Large Language Models (ChatGPT, Qwen-Turbo) for insightful financial analysis and personalized investment recommendations.  
- 🔄 **Workflow automation** – Built an end-to-end pipeline using Agently to automate data ingestion, processing, and reporting.  
- 🗃️ **Database management** – Designed and managed a MySQL relational database for efficient data storage and retrieval.  
- 🐍 **Python-MySQL integration** – Interacted with MySQL using Python to execute queries, store results, and perform data processing operations.  
- 🏗️ **Modular architecture** – Developed reusable modules for news crawling, SQL query execution, and AI-driven content generation.  
- ⚙️ **Dynamic SQL generation** – Utilized LLMs to create optimized SQL queries for database operations dynamically.  
- 🧵 **Chain of Thought (CoT) prompting** – Applied CoT prompting techniques to enhance report accuracy and logical consistency.  
- 🌐 **API integration** – Integrated cloud-based LLM APIs to enable real-time data processing and reporting.  


---

# **Breast Cancer Diagnosis using LSTM and XGBoost**
This project aims to predict breast cancer diagnosis based on the Wisconsin Breast Cancer dataset. It leverages both deep learning and traditional machine learning techniques to compare performance and optimize prediction accuracy. The pipeline includes data acquisition, preprocessing, model training, and evaluation.

*Technologies: Python, Pandas, TensorFlow, XGBoost, Scikit-learn, Bayesian Optimization, Matplotlib, Seaborn*

🚀 **Project Highlights**

- 📰 **Automated data acquisition** – Automatically downloads and processes the Wisconsin Breast Cancer dataset from Hugging Face.  
- 📊 **Feature selection** – Selects the most important features based on correlation analysis to improve model performance.  
- 🧠 **Deep learning (LSTM)** – Implements an LSTM-based neural network for accurate breast cancer diagnosis.  
- 🔄 **Machine learning (XGBoost)** – Builds an XGBoost model with Bayesian optimization for hyperparameter tuning.  
- 🗃️ **Data preprocessing** – Scales and reshapes data to fit machine learning and deep learning models efficiently.  
- 🏗️ **Pipeline automation** – Automates data preprocessing and model training using Scikit-learn pipelines.  
- ⚙️ **Bayesian hyperparameter optimization** – Uses Bayesian search to optimize XGBoost hyperparameters for improved accuracy.  
- 🌐 **Model evaluation** – Assesses model performance using accuracy metrics and visualizations.  
- 📈 **Feature importance analysis** – Visualizes the most influential features using XGBoost feature importance plots.

---

   Semantic Search and Question Answering System
Technologies: Python, OpenAI GPT-4, LangChain, LlamaIndex, Chroma, Recursive Text Splitting
This project implements a Semantic Search and Question Answering System that enables users to query a corpus of documents and receive concise, contextually accurate answers. It leverages Large Language Models (LLMs) such as OpenAI’s GPT-4, paired with vector-based retrieval for efficient semantic matching and summarization.
🚀 Project Highlights

📰 Document Ingestion and Processing – Loaded and processed text documents from local directories using efficient text loaders and recursive splitting techniques to prepare data for vectorization.
📊 Vector Database Integration – Embedded document chunks using OpenAI Embeddings and stored them in a vector database (Chroma) for semantic search.
🧠 LLM-Powered Summarization – Used MapReduceDocumentsChain for summarizing document chunks in a hierarchical manner, leveraging the GPT-4 model for context-aware summarization.
🔄 Retriever-Augmented Generation (RAG) – Implemented RAG chains for precise question answering by combining retrieved contexts with a structured prompt.
🧵 Custom Query Templates – Designed specialized prompts to guide the language model to provide concise answers, including options for language-specific responses (e.g., Chinese).
🗃️ Dynamic Query Retrieval – Utilized retrievers to fetch relevant chunks from the vector store based on similarity scores, ensuring accuracy and relevance in question answering.
🌐 Dual Framework Support – Explored the capabilities of both LangChain and LlamaIndex to test flexibility in semantic search, document retrieval, and model integration.
⚙️ High-Performance Workflow – Automated document loading, embedding, retrieval, summarization, and answering in a pipeline optimized for large-scale document processing.


